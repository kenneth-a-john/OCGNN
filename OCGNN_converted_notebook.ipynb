{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c4771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--dropout DROPOUT]\n",
      "                             [--nu NU] [--gpu GPU] [--seed SEED]\n",
      "                             [--module MODULE] [--n-worker N_WORKER]\n",
      "                             [--batch-size BATCH_SIZE] [--lr LR]\n",
      "                             [--normal-class NORMAL_CLASS]\n",
      "                             [--n-epochs N_EPOCHS] [--n-hidden N_HIDDEN]\n",
      "                             [--n-layers N_LAYERS]\n",
      "                             [--weight-decay WEIGHT_DECAY] [--early-stop]\n",
      "                             [--self-loop] [--norm]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/kennethjohn/Library/Jupyter/runtime/kernel-c2f3fa66-933a-4f28-8e22-6dc2fc6b7161.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3386: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from dgl.data import register_data_args\n",
    "import logging\n",
    "#import fire\n",
    "from optim import trainer, TUtrainer, AEtrainer\n",
    "from optim.loss import loss_function,init_center\n",
    "from datasets import dataloader,TUloader\n",
    "from networks.init import init_model\n",
    "import numpy as np\n",
    "import torch\n",
    "from dgl import random as dr\n",
    "\n",
    "def main(args):\n",
    "    if args.seed!=-1:\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        #torch.backends.cudnn.deterministic=True\n",
    "        dr.seed(args.seed)\n",
    "\n",
    "    checkpoints_path=f'./checkpoints/{args.dataset}+OC-{args.module}+bestcheckpoint.pt'\n",
    "    logging.basicConfig(filename=f\"./log/{args.dataset}+OC-{args.module}.log\",filemode=\"a\",format=\"%(asctime)s-%(name)s-%(levelname)s-%(message)s\",level=logging.INFO)\n",
    "    logger=logging.getLogger('OCGNN')\n",
    "\n",
    "\n",
    "#     print('model:',args.module)\n",
    "#     print('seed:',args.seed)\n",
    "\n",
    "    if args.dataset in 'PROTEINS_full'+'ENZYMES'+'FRANKENSTEIN':\n",
    "        train_loader, val_loader, test_loader, input_dim, label_dim=TUloader.loader(args)\n",
    "        model=init_model(args,input_dim)\n",
    "        model=TUtrainer.train(args,logger,train_loader,model,val_dataset=val_loader,path=checkpoints_path)\n",
    "        # auc,ap,f1,acc,precision,recall,_= multi_graph_evaluate(args,checkpoints_path,\n",
    "        #     model, data_center,test_loader,radius,mode='test')\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "        # print(\"Test AUROC {:.4f} | Test AUPRC {:.4f}\".format(auc,ap))\n",
    "        # print(f'Test f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "        # logger.info(\"Current epoch: {:d} Test AUROC {:.4f} | Test AUPRC {:.4f}\".format(epoch,auc,ap))\n",
    "        # logger.info(f'Test f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "        # logger.info('\\n')\n",
    "    else:\n",
    "        data=dataloader.loader(args)\n",
    "        model=init_model(args,data['input_dim'])\n",
    "        if args.module != 'GAE':\n",
    "            model=trainer.train(args,logger,data,model,checkpoints_path)\n",
    "        else:\n",
    "            model=AEtrainer.train(args,logger,data,model,checkpoints_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='OCGNN')\n",
    "    register_data_args(parser)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5,\n",
    "            help=\"dropout probability\")\n",
    "    parser.add_argument(\"--nu\", type=float, default=0.2,\n",
    "            help=\"hyperparameter nu (must be 0 < nu <= 1)\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "            help=\"gpu\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=52,\n",
    "            help=\"random seed, -1 means dont fix seed\")\n",
    "    parser.add_argument(\"--module\", type=str, default='GraphSAGE',\n",
    "            help=\"GCN/GAT/GIN/GraphSAGE/GAE\")\n",
    "    parser.add_argument('--n-worker', type=int,default=1,\n",
    "            help='number of workers when dataloading')\n",
    "    parser.add_argument('--batch-size', type=int,default=128,\n",
    "            help='batch size')\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3,\n",
    "            help=\"learning rate\")\n",
    "    parser.add_argument(\"--normal-class\", type=int, default=0,\n",
    "            help=\"normal class\")\n",
    "    parser.add_argument(\"--n-epochs\", type=int, default=5000,\n",
    "            help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--n-hidden\", type=int, default=32,\n",
    "            help=\"number of hidden gnn units\")\n",
    "    parser.add_argument(\"--n-layers\", type=int, default=2,\n",
    "            help=\"number of hidden gnn layers\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\n",
    "            help=\"Weight for L2 loss\")\n",
    "    parser.add_argument('--early-stop', action='store_true', default=False,\n",
    "                        help=\"indicates whether to use early stop or not\")\n",
    "    parser.add_argument(\"--self-loop\", action='store_true',\n",
    "            help=\"graph self-loop (default=False)\")\n",
    "    parser.add_argument(\"--norm\", action='store_true',\n",
    "            help=\"graph normalization (default=False)\")\n",
    "    parser.set_defaults(self_loop=True)\n",
    "    parser.set_defaults(norm=False)\n",
    "    args = parser.parse_args()\n",
    "    if args.module=='GCN':\n",
    "        #args.self_loop=True\n",
    "        args.norm=True\n",
    "    if args.module=='GAE':\n",
    "        args.lr=0.002\n",
    "        args.dropout=0.\n",
    "        args.weight_decay=0.\n",
    "        # args.n_hidden=32\n",
    "    #     args.self_loop=True\n",
    "    # if args.module=='GraphSAGE':\n",
    "    #     args.self_loop=True\n",
    "\n",
    "    if args.dataset in ('citeseer' + 'reddit'):\n",
    "        args.normal_class=3\n",
    "    if args.dataset in ('cora' + 'pubmed'):\n",
    "        args.normal_class=2\n",
    "    if args.dataset in 'TU_PROTEINS_full':\n",
    "        args.normal_class=0\n",
    "\n",
    "    #fire.Fire(main(args))\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dgl.data import register_data_args\n",
    "import time\n",
    "from datasets.dataloader import emb_dataloader\n",
    "from utils.evaluate import baseline_evaluate\n",
    "import fire\n",
    "import logging\n",
    "from embedding.get_embedding import embedding\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score,precision_score,recall_score,average_precision_score,roc_auc_score,roc_curve\n",
    "\n",
    "def main():\n",
    "\tparser = argparse.ArgumentParser(description='baseline')\n",
    "\tregister_data_args(parser)\n",
    "\tparser.add_argument(\"--mode\", type=str, default='A',choices=['A','AX','X'],\n",
    "\t\t\thelp=\"dropout probability\")\n",
    "\tparser.add_argument(\"--seed\", type=int, default=-1,\n",
    "            help=\"random seed, -1 means dont fix seed\")\n",
    "\tparser.add_argument(\"--emb-method\", type=str, default='DeepWalk',\n",
    "\t\t\thelp=\"embedding methods: DeepWalk, Node2Vec, LINE, SDNE, Struc2Vec\")  \n",
    "\tparser.add_argument(\"--ad-method\", type=str, default='OCSVM',\n",
    "\t\t\thelp=\"embedding methods: PCA,OCSVM,IF,AE\")     \n",
    "\tparser.add_argument(\"--normal-class\", type=int, default=0,\n",
    "            help=\"normal class\")       \n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\tif args.dataset in ('citeseer' + 'reddit'):\n",
    "\t\targs.normal_class=3\n",
    "\tif args.dataset in ('cora' + 'pubmed'):\n",
    "\t\targs.normal_class=2\n",
    "\tif args.dataset in 'TU_PROTEINS_full':\n",
    "\t\targs.normal_class=0\n",
    "\n",
    "\tif args.seed!=-1:\n",
    "\t\tnp.random.seed(args.seed)\n",
    "\t\ttorch.manual_seed(args.seed)\n",
    "\t\ttorch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\tlogging.basicConfig(filename=f\"./log/{args.dataset}-twostage-{args.seed}.log\",filemode=\"a\",format=\"%(asctime)s-%(name)s-%(levelname)s-%(message)s\",level=logging.INFO)\n",
    "\tlogger=logging.getLogger('baseline')\n",
    "\n",
    "\n",
    "\tdatadict=emb_dataloader(args)\n",
    "\n",
    "\tif args.mode=='X':\n",
    "\t\tdata=datadict['features']\n",
    "\t\t#print('X shape',data.shape)\n",
    "\telse:\n",
    "\t\tt0 = time.time()\n",
    "\t\tembeddings=embedding(args,datadict)\n",
    "\t\tdur1=time.time() - t0\n",
    "\t\t\n",
    "\t\tif args.mode=='A':\n",
    "\t\t\tdata=embeddings\n",
    "\t\t\t#print('A shape',data.shape)\n",
    "\t\tif args.mode=='AX':\n",
    "\t\t\tdata=np.concatenate((embeddings,datadict['features']),axis=1)\n",
    "\t\t\t#print('AX shape',data.shape)\n",
    "\n",
    "\tlogger.debug(f'data shape: {data.shape}')\n",
    "\n",
    "\tif args.ad_method=='OCSVM':\n",
    "\t\tclf = OCSVM(contamination=0.1)\n",
    "\tif args.ad_method=='IF':\n",
    "\t\tclf = IForest(n_estimators=100,contamination=0.1,n_jobs=-1,behaviour=\"new\")\n",
    "\tif args.ad_method=='PCA':\n",
    "\t\tclf = PCA(contamination=0.1)\n",
    "\tif args.ad_method=='AE':\n",
    "\t\tclf = AutoEncoder(contamination=0.1)\n",
    "\t\n",
    "\n",
    "\n",
    "\tt1 = time.time()\n",
    "\tclf.fit(data[datadict['train_mask']])\n",
    "\tdur2=time.time() - t1\n",
    "\n",
    "\tprint('traininig time:', dur1+dur2)\n",
    "\n",
    "\tlogger.info('\\n')\n",
    "\tlogger.info('\\n')\n",
    "\tlogger.info(f'Parameters dataset:{args.dataset} datamode:{args.mode} ad-method:{args.ad_method} emb-method:{args.emb_method}')\n",
    "\tlogger.info('-------------Evaluating Validation Results--------------')\n",
    "\n",
    "\tt2 = time.time()\n",
    "\ty_pred_val=clf.predict(data[datadict['val_mask']])\n",
    "\ty_score_val=clf.decision_function(data[datadict['val_mask']])\n",
    "\tauc,ap,f1,acc,precision,recall=baseline_evaluate(datadict,y_pred_val,y_score_val,val=True)\n",
    "\tdur3=time.time() - t2\n",
    "\tprint('infer time:', dur3)\n",
    "\n",
    "\tlogger.info(f'AUC:{round(auc,4)},AP:{round(ap,4)}')\n",
    "\tlogger.info(f'f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "\n",
    "\tlogger.info('-------------Evaluating Test Results--------------')\n",
    "\ty_pred_test=clf.predict(data[datadict['test_mask']])\n",
    "\ty_score_test=clf.decision_function(data[datadict['test_mask']])\n",
    "\tauc,ap,f1,acc,precision,recall=baseline_evaluate(datadict,y_pred_test,y_score_test,val=False)\n",
    "\tlogger.info(f'AUC:{round(auc,4)},AP:{round(ap,4)}')\n",
    "\tlogger.info(f'f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #print(args)\n",
    "\t#main()\n",
    "    fire.Fire(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "#from dgl.contrib.sampling.sampler import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score,precision_score,recall_score,average_precision_score,roc_auc_score,roc_curve\n",
    "\n",
    "\n",
    "from optim.loss import EarlyStopping\n",
    "\n",
    "#choose mode of GAE, A means kipf's GAE, X means AE with considering network structure, AX means Ding's Dominant model.\n",
    "# GAE_mode can be selected form 'AX', 'A' or 'X'.\n",
    "GAE_mode='AX'\n",
    "\n",
    "\n",
    "def train(args,logger,data,model,path):\n",
    "\n",
    "    checkpoints_path=path\n",
    "\n",
    "    # logging.basicConfig(filename=f\"./log/{args.dataset}+OC-{args.module}.log\",filemode=\"a\",format=\"%(asctime)s-%(name)s-%(levelname)s-%(message)s\",level=logging.INFO)\n",
    "    # logger=logging.getLogger('OCGNN')\n",
    "    #loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "    # use optimizer AdamW\n",
    "    logger.info('Start training')\n",
    "    logger.info(f'dropout:{args.dropout}, nu:{args.nu},seed:{args.seed},lr:{args.lr},self-loop:{args.self_loop},norm:{args.norm}')\n",
    "\n",
    "    logger.info(f'n-epochs:{args.n_epochs}, n-hidden:{args.n_hidden},n-layers:{args.n_layers},weight-decay:{args.weight_decay}')\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "    if args.early_stop:\n",
    "        stopper = EarlyStopping(patience=100)\n",
    "    # initialize data center\n",
    "\n",
    "    adj=data['g'].adjacency_matrix().to_dense().cuda()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    #train_inputs=data['features']\n",
    "    #print('adj dim',adj[data['train_mask']].size())\n",
    "\n",
    "    dur = []\n",
    "    model.train()\n",
    "\n",
    "    #创立矩阵以存储结果曲线\n",
    "    arr_epoch=np.arange(args.n_epochs)\n",
    "    arr_loss=np.zeros(args.n_epochs)\n",
    "    arr_valauc=np.zeros(args.n_epochs)\n",
    "    arr_testauc=np.zeros(args.n_epochs)\n",
    "\n",
    "    for epoch in range(args.n_epochs):\n",
    "        #model.train()\n",
    "        #if epoch %5 == 0:\n",
    "        t0 = time.time()\n",
    "        # forward\n",
    "\n",
    "        z,re_x,re_adj= model(data['g'],data['features'])\n",
    "\n",
    "        loss=Recon_loss(re_x,re_adj,adj,data['features'],data['train_mask'],loss_fn,GAE_mode)\n",
    "\n",
    "        #保存训练loss\n",
    "        arr_loss[epoch]=loss.item()\n",
    "        #\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur=time.time() - t0\n",
    "        \n",
    "        auc,ap,val_loss=fixed_graph_evaluate(args,model,data,adj,data['val_mask'])\n",
    "        #保存验证集AUC\n",
    "        arr_valauc[epoch]=auc\n",
    "        #保存验证集AUC\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Val AUROC {:.4f} | Val loss {:.4f} | \"\n",
    "              \"ETputs(KTEPS) {:.2f}\". format(epoch, np.mean(dur), loss.item()*100000,\n",
    "                                            auc,val_loss, data['n_edges'] / np.mean(dur) / 1000))\n",
    "        if args.early_stop:\n",
    "            if stopper.step(auc,val_loss.item(), model,epoch,checkpoints_path):   \n",
    "                break\n",
    "\n",
    "    if args.early_stop:\n",
    "        print('loading model before testing.')\n",
    "        model.load_state_dict(torch.load(checkpoints_path))\n",
    "\n",
    "        #if epoch%100 == 0:\n",
    "    \n",
    "    auc,ap,_ = fixed_graph_evaluate(args,model,data,adj,data['test_mask'])\n",
    "    test_dur=0\n",
    "    #保存测试集AUC\n",
    "    arr_testauc[epoch]=auc\n",
    "    #保存测试集AUC\n",
    "    print(\"Test Time {:.4f} | Test AUROC {:.4f} | Test AUPRC {:.4f}\".format(test_dur,auc,ap))\n",
    "    #print(f'Test f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "    #logger.info(\"Current epoch: {:d} Test AUROC {:.4f} | Test AUPRC {:.4f}\".format(epoch,auc,ap))\n",
    "    #logger.info(f'Test f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "    #logger.info('\\n')\n",
    "\n",
    "    #np.savez('Dom3.npz',epoch=arr_epoch,loss=arr_loss,valauc=arr_valauc,testauc=arr_testauc)\n",
    "\n",
    "    return model\n",
    "\n",
    "def Recon_loss(re_x,re_adj,adj,x,mask,loss_fn,mode):\n",
    "    #S_loss: structure loss A_loss: Attribute loss\n",
    "    if mode=='A':\n",
    "        return loss_fn(re_x[mask], x[mask])\n",
    "    if mode=='X':\n",
    "        return loss_fn(re_x[mask], x[mask]) \n",
    "    if mode=='AX':     \n",
    "        return 0.5*loss_fn(re_x[mask], x[mask]) + 0.5*loss_fn(re_adj[mask], adj[mask])\n",
    "\n",
    "def anomaly_score(re_x,re_adj,adj,x,mask,loss_fn,mode):\n",
    "    if mode=='A':\n",
    "        S_scores=F.mse_loss(re_adj[mask], adj[mask], reduction='none')\n",
    "        return torch.mean(S_scores,1)\n",
    "    if mode=='X':\n",
    "        A_scores=F.mse_loss(re_x[mask], x[mask], reduction='none')\n",
    "        return torch.mean(A_scores,1)\n",
    "    if mode=='AX': \n",
    "        A_scores=F.mse_loss(re_x[mask], x[mask], reduction='none')\n",
    "        S_scores=F.mse_loss(re_adj[mask], adj[mask], reduction='none')\n",
    "        return 0.5*torch.mean(A_scores,1)+0.5*torch.mean(S_scores,1)\n",
    "\n",
    "def fixed_graph_evaluate(args,model,data,adj,mask):\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        labels = data['labels'][mask]\n",
    "        \n",
    "        loss_mask=mask.bool() & data['labels'].bool()\n",
    "\n",
    "        #test_t0=time.time()\n",
    "        z,re_x,re_adj= model(data['g'],data['features'])\n",
    "\n",
    "        loss=Recon_loss(re_x,re_adj, adj, data['features'],loss_mask,loss_fn,GAE_mode)\n",
    "        #test_dur = time.time()-test_t0\n",
    "        #print(\"Test Time {:.4f}\".format(test_dur))\n",
    "        #print(recon[data['val_mask']].size())\n",
    "        scores=anomaly_score(re_x,re_adj, adj, data['features'],mask,loss_fn,GAE_mode)\n",
    "        \n",
    "        # A_scores=F.mse_loss(re_x[mask], data['features'][mask], reduction='none')\n",
    "        # S_scores=F.mse_loss(re_adj[mask], adj[mask], reduction='none')\n",
    "        # scores=torch.mean(A_scores,1)+torch.mean(S_scores,1)\n",
    "\n",
    "        labels=labels.cpu().numpy()\n",
    "        # print(labels.shape)\n",
    "        # print(scores.shape)\n",
    "        #dist=dist.cpu().numpy()\n",
    "        scores=scores.cpu().numpy()\n",
    "        #pred=thresholding(scores,0)\n",
    "        #print('scores.shape',scores)\n",
    "        auc=roc_auc_score(labels, scores)\n",
    "        ap=average_precision_score(labels, scores)\n",
    "\n",
    "        # acc=accuracy_score(labels,pred)\n",
    "        # recall=recall_score(labels,pred)\n",
    "        # precision=precision_score(labels,pred)\n",
    "        # f1=f1_score(labels,pred)\n",
    "\n",
    "\n",
    "    return auc,ap,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ecb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "def loss_function(nu,data_center,outputs,radius=0,mask=None):\n",
    "    dist,scores=anomaly_score(data_center,outputs,radius,mask)\n",
    "    loss = radius ** 2 + (1 / nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
    "    return loss,dist,scores\n",
    "\n",
    "def anomaly_score(data_center,outputs,radius=0,mask= None):\n",
    "    if mask == None:\n",
    "        dist = torch.sum((outputs - data_center) ** 2, dim=1)\n",
    "    else:\n",
    "        dist = torch.sum((outputs[mask] - data_center) ** 2, dim=1)\n",
    "    # c=data_center.repeat(outputs[mask].size()[0],1)\n",
    "    # res=outputs[mask]-c\n",
    "    # res=torch.mean(res, 1, keepdim=True)\n",
    "    # dist=torch.diag(torch.mm(res,torch.transpose(res, 0, 1)))\n",
    "\n",
    "    scores = dist - radius ** 2\n",
    "    return dist,scores\n",
    "\n",
    "def init_center(args,input_g,input_feat, model, eps=0.001):\n",
    "    \"\"\"Initialize hypersphere center c as the mean from an initial forward pass on the data.\"\"\"\n",
    "    if args.gpu < 0:\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda:%d' % args.gpu)\n",
    "    n_samples = 0\n",
    "    c = torch.zeros(args.n_hidden, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs= model(input_g,input_feat)\n",
    "\n",
    "        # get the inputs of the batch\n",
    "\n",
    "        n_samples = outputs.shape[0]\n",
    "        c =torch.sum(outputs, dim=0)\n",
    "\n",
    "    c /= n_samples\n",
    "\n",
    "    # If c_i is too close to 0, set to +-eps. Reason: a zero unit can be trivially matched with zero weights.\n",
    "    c[(abs(c) < eps) & (c < 0)] = -eps\n",
    "    c[(abs(c) < eps) & (c > 0)] = eps\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def get_radius(dist: torch.Tensor, nu: float):\n",
    "    \"\"\"Optimally solve for radius R via the (1-nu)-quantile of distances.\"\"\"\n",
    "    radius=np.quantile(np.sqrt(dist.clone().data.cpu().numpy()), 1 - nu)\n",
    "    # if radius<0.1:\n",
    "    #     radius=0.1\n",
    "    return radius\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_epoch = None\n",
    "        self.lowest_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, acc,loss, model,epoch,path):\n",
    "        score = acc\n",
    "        cur_loss=loss\n",
    "        if (self.best_score is None) or (self.lowest_loss is None):\n",
    "        #if self.lowest_loss is None:\n",
    "            self.best_score = score\n",
    "            self.lowest_loss = cur_loss\n",
    "            self.save_checkpoint(acc,loss,model,path)\n",
    "        #elif cur_loss > self.lowest_loss:\n",
    "        elif (score < self.best_score) and (cur_loss > self.lowest_loss):\n",
    "            self.counter += 1\n",
    "            if self.counter >= 0.8*(self.patience):\n",
    "                print(f'Warning: EarlyStopping soon: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.lowest_loss = cur_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(acc,loss,model,path)\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, acc,loss,model,path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        print('model saved. loss={:.4f} AUC={:.4f}'. format(loss,acc))\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa000c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "#from dgl.contrib.sampling.sampler import NeighborSampler\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from optim.loss import loss_function,init_center,get_radius,EarlyStopping\n",
    "\n",
    "from utils import fixed_graph_evaluate\n",
    "\n",
    "def train(args,logger,data,model,path):\n",
    "    if args.gpu < 0:\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda:%d' % args.gpu)\n",
    "    checkpoints_path=path\n",
    "\n",
    "    # logging.basicConfig(filename=f\"./log/{args.dataset}+OC-{args.module}.log\",filemode=\"a\",format=\"%(asctime)s-%(name)s-%(levelname)s-%(message)s\",level=logging.INFO)\n",
    "    # logger=logging.getLogger('OCGNN')\n",
    "    #loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "    # use optimizer AdamW\n",
    "    logger.info('Start training')\n",
    "    logger.info(f'dropout:{args.dropout}, nu:{args.nu},seed:{args.seed},lr:{args.lr},self-loop:{args.self_loop},norm:{args.norm}')\n",
    "\n",
    "    logger.info(f'n-epochs:{args.n_epochs}, n-hidden:{args.n_hidden},n-layers:{args.n_layers},weight-decay:{args.weight_decay}')\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "    if args.early_stop:\n",
    "        stopper = EarlyStopping(patience=100)\n",
    "    # initialize data center\n",
    "\n",
    "    input_feat=data['features']\n",
    "    input_g=data['g']\n",
    "\n",
    "    data_center= init_center(args,input_g,input_feat, model)\n",
    "    radius=torch.tensor(0, device=device)# radius R initialized with 0 by default.\n",
    "\n",
    "\n",
    "    #创立矩阵以存储结果曲线\n",
    "    arr_epoch=np.arange(args.n_epochs)\n",
    "    arr_loss=np.zeros(args.n_epochs)\n",
    "    arr_valauc=np.zeros(args.n_epochs)\n",
    "    arr_testauc=np.zeros(args.n_epochs)\n",
    "\n",
    "    dur = []\n",
    "    model.train()\n",
    "    for epoch in range(args.n_epochs):\n",
    "        #model.train()\n",
    "        #if epoch %5 == 0:\n",
    "        t0 = time.time()\n",
    "        # forward\n",
    "\n",
    "        outputs= model(input_g,input_feat)\n",
    "        #print('model:',args.module)\n",
    "        #print('output size:',outputs.size())\n",
    "\n",
    "        loss,dist,_=loss_function(args.nu, data_center,outputs,radius,data['train_mask'])\n",
    "        #保存训练loss\n",
    "        arr_loss[epoch]=loss.item()\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch>= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        radius.data=torch.tensor(get_radius(dist, args.nu), device=device)\n",
    "\n",
    "\n",
    "        auc,ap,f1,acc,precision,recall,val_loss = fixed_graph_evaluate(args,checkpoints_path, model, data_center,data,radius,data['val_mask'])\n",
    "        #保存验证集AUC\n",
    "        arr_valauc[epoch]=auc\n",
    "        #保存测试集AUC\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Train Loss {:.4f} | Val Loss {:.4f} | Val AUROC {:.4f} | \"\n",
    "              \"ETputs(KTEPS) {:.2f}\". format(epoch, np.mean(dur), loss.item()*100000,\n",
    "                                            val_loss.item()*100000, auc, data['n_edges'] / np.mean(dur) / 1000))\n",
    "        print(f'Val f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "        if args.early_stop:\n",
    "            if stopper.step(auc,val_loss.item(), model,epoch,checkpoints_path):\n",
    "                break\n",
    "\n",
    "    if args.early_stop:\n",
    "        print('loading model before testing.')\n",
    "        model.load_state_dict(torch.load(checkpoints_path))\n",
    "\n",
    "\n",
    "    auc,ap,f1,acc,precision,recall,loss = fixed_graph_evaluate(args,checkpoints_path,model, data_center,data,radius,data['test_mask'])\n",
    "    test_dur = 0\n",
    "    #保存测试集AUC\n",
    "    arr_testauc[epoch]=auc\n",
    "    #保存测试集AUC\n",
    "    print(\"Test Time {:.4f} | Test AUROC {:.4f} | Test AUPRC {:.4f}\".format(test_dur,auc,ap))\n",
    "    print(f'Test f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "    # logger.info(\"Current epoch: {:d} Test AUROC {:.4f} | Test AUPRC {:.4f}\".format(epoch,auc,ap))\n",
    "    # logger.info(f'Test f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "    # logger.info('\\n')\n",
    "\n",
    "    #np.savez('SAGE-2.npz',epoch=arr_epoch,loss=arr_loss,valauc=arr_valauc,testauc=arr_testauc)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc852df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "#from dgl.contrib.sampling.sampler import NeighborSampler\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from optim.loss import loss_function,init_center,get_radius,EarlyStopping\n",
    "\n",
    "from utils.evaluate import multi_graph_evaluate\n",
    "\n",
    "def train(args, logger,dataset, model, val_dataset=None,path=None):\n",
    "    '''\n",
    "    training function\n",
    "    '''\n",
    "    checkpoints_path=path\n",
    "\n",
    "    #loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "    # use optimizer AdamW\n",
    "    logger.info('Start training')\n",
    "    logger.info(f'dropout:{args.dropout}, nu:{args.nu},seed:{args.seed},lr:{args.lr},self-loop:{args.self_loop},norm:{args.norm}')\n",
    "\n",
    "    logger.info(f'n-epochs:{args.n_epochs}, n-hidden:{args.n_hidden},n-layers:{args.n_layers},weight-decay:{args.weight_decay}')\n",
    "    \n",
    "    dataloader = dataset\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "    # optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "    #                                     model.parameters()), lr=0.001)\n",
    "    if args.early_stop:\n",
    "        stopper = EarlyStopping(patience=100)\n",
    "    #early_stopping_logger = {\"best_epoch\": -1, \"val_acc\": -1}\n",
    "\n",
    "\n",
    "    #data_center= init_center(args,input_g,input_feat, model)\n",
    "    data_center= torch.zeros(args.n_hidden, device=f'cuda:{args.gpu}')\n",
    "    radius=torch.tensor(0, device=f'cuda:{args.gpu}')# radius R initialized with 0 by default.\n",
    "    #loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    model.train()\n",
    "    for epoch in range(args.n_epochs):\n",
    "        begin_time = time.time()\n",
    "        # accum_correct = 0\n",
    "        # total = 0\n",
    "        print(\"EPOCH ###### {} ######\".format(epoch))\n",
    "        computation_time = 0.0\n",
    "        for (batch_idx, (batch_graph, graph_labels)) in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                for (key, value) in batch_graph.ndata.items():\n",
    "                    batch_graph.ndata[key] = value.cuda()\n",
    "                #graph_labels = graph_labels.cuda()\n",
    "            #print(batch_graph)\n",
    "            train_mask=~batch_graph.ndata['node_labels'].bool().squeeze()\n",
    "            model.zero_grad()\n",
    "            compute_start = time.time()\n",
    "\n",
    "            normlizing = nn.BatchNorm1d(batch_graph.ndata['node_attr'].shape[1], affine=False).cuda()\n",
    "            input_attr=normlizing(batch_graph.ndata['node_attr'])\n",
    "\n",
    "            # normlizing = nn.InstanceNorm1d(batch_graph.ndata['node_attr'].shape[1], affine=False).cuda()\n",
    "            # input_attr=normlizing(batch_graph.ndata['node_attr'].unsqueeze(1)).squeeze()\n",
    "\n",
    "            #data_center= init_center(args,batch_graph,batch_graph.ndata['node_attr'], model)\n",
    "            #print('data_center',data_center)\n",
    "            outputs = model(batch_graph,input_attr)\n",
    "            # print('outputs mean',outputs.mean())\n",
    "            # print('outputs std',outputs.std())\n",
    "\n",
    "            #loss = loss_fn(outputs, batch_graph.ndata['node_labels'].float())\n",
    "            loss,dist,score=loss_function(args.nu, data_center,outputs,radius,train_mask)\n",
    "            #if batch_idx<=3:\n",
    "                #print(dist)\n",
    "                # print(score)\n",
    "            # print('dist mean',dist.mean())\n",
    "            # print('dist std',dist.std())\n",
    "            loss.backward()\n",
    "            batch_compute_time = time.time() - compute_start\n",
    "            computation_time += batch_compute_time\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            #radius.data=torch.tensor(get_radius(dist, args.nu), device=f'cuda:{args.gpu}')\n",
    "            print('RRR',radius.data)\n",
    "            print(\"Epoch {:05d},loss {:.4f} with {}-th batch time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), batch_idx, computation_time))\n",
    "        #train_accu = accum_correct / total\n",
    "        #print(\"train loss for this epoch {} is {}%\".format(epoch,train_accu * 100))\n",
    "        elapsed_time = time.time() - begin_time\n",
    "        #print(\"Epoch {:05d}, loss {:.4f} with epoch time(s) {:.4f}\".format(epoch,loss.item(), elapsed_time))\n",
    "        if val_dataset is not None:\n",
    "            auc,ap,f1,acc,precision,recall,loss = multi_graph_evaluate(args,checkpoints_path, model, data_center,val_dataset,radius,'val')\n",
    "            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Val AUROC {:.4f} | Val F1 {:.4f} | Val ACC {:.4f} | \". format(\n",
    "                epoch, elapsed_time, loss.item()*100000, auc,f1,acc))\n",
    "            torch.cuda.empty_cache()\n",
    "            if args.early_stop:\n",
    "                if stopper.step(auc,loss.item()*100000, model,epoch,checkpoints_path):  \n",
    "                    print(\"best epoch is EPOCH {}, val_auc is {}%\".format(stopper.best_epoch,\n",
    "                                                        stopper.best_score)) \n",
    "                    break\n",
    "\n",
    "    # auc,ap,f1,acc,precision,recall,_ = multi_graph_evaluate(args,checkpoints_path, model, data_center,data,radius,'test')\n",
    "    # torch.cuda.empty_cache()\n",
    "    # print(\"Test AUROC {:.4f} | Test AUPRC {:.4f}\".format(auc,ap))\n",
    "    # print(f'Test f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "    # logger.info(\"Current epoch: {:d} Test AUROC {:.4f} | Test AUPRC {:.4f}\".format(epoch,auc,ap))\n",
    "    # logger.info(f'Test f1:{round(f1,4)},acc:{round(acc,4)},pre:{round(precision,4)},recall:{round(recall,4)}')\n",
    "    # logger.info('\\n')\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import load_data, tu\n",
    "from dgl import DGLGraph, transforms\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "import networkx as nx\n",
    "from datasets.prepocessing import one_class_processing\n",
    "\n",
    "def loader(args):\n",
    "    # load and preprocess dataset\n",
    "    \n",
    "    data = load_data(args)\n",
    "\n",
    "    print(f'normal_class is {args.normal_class}')\n",
    "\n",
    "    labels,train_mask,val_mask,test_mask=one_class_processing(data,args.normal_class,args)\n",
    "    features= data[0].ndata['feat']\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.LongTensor(labels.numpy())\n",
    "    train_mask = torch.BoolTensor(train_mask)\n",
    "    val_mask = torch.BoolTensor(val_mask)\n",
    "    test_mask = torch.BoolTensor(test_mask)\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = data.num_labels\n",
    "    n_edges = data[0].number_of_edges()\n",
    "    print(\"args.gpu: \", args)\n",
    "    print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "              train_mask.sum().item(),\n",
    "              val_mask.sum().item(),\n",
    "              test_mask.sum().item()))\n",
    "\n",
    "    if args.gpu < 0:\n",
    "        cuda = False\n",
    "    else:\n",
    "        cuda = True\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    # graph preprocess and calculate normalization factor\n",
    "    g = data[0]\n",
    "    \n",
    "        \n",
    "    # add self loop\n",
    "    if args.self_loop:\n",
    "        g.remove_edges(nx.selfloop_edges(g))\n",
    "        #g=transform.remove_self_loop(g)\n",
    "        #if args.module!='GraphSAGE':\n",
    "        g.add_edges_from(zip(g.nodes(), g.nodes()))\n",
    "\n",
    "    g = DGLGraph(g)\n",
    "    g = g.to(torch.device('cuda:0'))\n",
    "    n_edges = g.number_of_edges()\n",
    "    if args.norm:\n",
    "        \n",
    "        # normalization\n",
    "        degs = g.in_degrees().float()\n",
    "        norm = torch.pow(degs, -0.5)\n",
    "        norm[torch.isinf(norm)] = 0\n",
    "        if cuda:\n",
    "            norm = norm.cuda()\n",
    "        g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    datadict={'g':g,'features':features,'labels':labels,'train_mask':train_mask,\n",
    "        'val_mask':val_mask,'test_mask': test_mask,'input_dim':in_feats,'n_classes':n_classes,'n_edges':n_edges}\n",
    "\n",
    "    return datadict\n",
    "\n",
    "def emb_dataloader(args):\n",
    "    # load and preprocess dataset\n",
    "    data = load_data(args)\n",
    "    normal_class=args.normal_class\n",
    "    labels,train_mask,val_mask,test_mask=one_class_processing(data,normal_class,args)\n",
    "\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    train_mask = torch.BoolTensor(train_mask)\n",
    "    val_mask = torch.BoolTensor(val_mask)\n",
    "    test_mask = torch.BoolTensor(test_mask)\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = data.num_labels\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "              train_mask.sum().item(),\n",
    "              val_mask.sum().item(),\n",
    "              test_mask.sum().item()))\n",
    "\n",
    "    g = data.graph\n",
    "\n",
    "\n",
    "    datadict={'g':g,'features':features,'labels':labels,'train_mask':train_mask,\n",
    "        'val_mask':val_mask,'test_mask': test_mask,'in_feats':in_feats,'n_classes':n_classes,'n_edges':n_edges}\n",
    "\n",
    "    return datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def one_class_processing(data,normal_class:int,args=None):\n",
    "    labels,normal_idx,abnormal_idx=one_class_labeling(data[0].ndata['label'],normal_class)\n",
    "    return one_class_masking(args,data,labels,normal_idx,abnormal_idx)\n",
    "\n",
    "\n",
    "def one_class_labeling(labels,normal_class:int):\n",
    "    normal_idx=np.where(labels==normal_class)[0]\n",
    "    abnormal_idx=np.where(labels!=normal_class)[0]\n",
    "\n",
    "    labels[normal_idx]=0\n",
    "    labels[abnormal_idx]=1\n",
    "    np.random.shuffle(normal_idx)\n",
    "    np.random.shuffle(abnormal_idx)\n",
    "    labels = tf.cast(labels, dtype=tf.bool)\n",
    "    return labels, normal_idx, abnormal_idx\n",
    "\n",
    "#训练集60%正常、验证集15%正常、测试集25%正常，验证集测试集中的正常异常样本1:1\n",
    "def one_class_masking(args,data,labels,normal_idx,abnormal_idx):\n",
    "\ttrain_mask=np.zeros(labels.shape,dtype='bool')\n",
    "\tval_mask=np.zeros(labels.shape,dtype='bool')\n",
    "\ttest_mask=np.zeros(labels.shape,dtype='bool')\n",
    "\t\n",
    "\tif args.dataset=='reddit':\n",
    "\t\ttrain_mask=np.logical_and(data.train_mask,~labels)\n",
    "\t\tval_mask=masking_reddit(data.val_mask,labels)\n",
    "\t\ttest_mask=masking_reddit(data.test_mask,labels)\n",
    "\telse:\n",
    "\t\ttrain_mask[normal_idx[:int(0.6*normal_idx.shape[0])]]=1\n",
    "\n",
    "\t\tval_mask[normal_idx[int(0.6*normal_idx.shape[0]):int(0.75*normal_idx.shape[0])]]=1\n",
    "\t\tval_mask[abnormal_idx[:int(0.15*normal_idx.shape[0])]]=1\n",
    "\n",
    "\t\ttest_mask[normal_idx[int(0.75*normal_idx.shape[0]):]]=1\n",
    "\t\ttest_mask[abnormal_idx[-int(0.25*normal_idx.shape[0]):]]=1\n",
    "\n",
    "\treturn labels,train_mask,val_mask,test_mask  \n",
    "\n",
    "def masking_reddit(mask,labels):\n",
    "\n",
    "\tnormal=np.logical_and(~labels,mask)\n",
    "\tabnormal=np.logical_and(labels,mask)\n",
    "\tidx=np.where(abnormal==1)[0]\n",
    "\tnp.random.shuffle(idx)\n",
    "\tabnormal[idx[:-normal.sum()]]=0\n",
    "\tmask=np.logical_or(normal,abnormal)\n",
    "\treturn mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "全部节点都用于测试集分类\n",
    "def one_class_masking(labels,normal_idx,abnormal_idx):\n",
    "    train_mask=np.zeros(labels.shape)\n",
    "    val_mask=np.zeros(labels.shape)\n",
    "    test_mask=np.zeros(labels.shape)\n",
    "\n",
    "    train_mask[normal_idx[:int(0.7*normal_idx.shape[0])]]=1\n",
    "\n",
    "    val_mask[normal_idx[int(0.7*normal_idx.shape[0]):int(0.8*normal_idx.shape[0])]]=1\n",
    "    val_mask[abnormal_idx[:int(0.3*abnormal_idx.shape[0])]]=1\n",
    "\n",
    "    test_mask[normal_idx[int(0.8*normal_idx.shape[0]):]]=1\n",
    "    test_mask[abnormal_idx[int(0.3*abnormal_idx.shape[0]):]]=1\n",
    "\n",
    "    return labels,train_mask,val_mask,test_mask  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import load_data, tu\n",
    "from dgl import DGLGraph, transforms\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# add self loop for TU dataset, other datasets haven't been tested. \n",
    "def pre_process(args,dataset):\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        #print(dataset.graph_lists[i])\n",
    "        #make labels become 0 or 1, other label is not our need.\n",
    "        #dataset.graph_lists[i].ndata\n",
    "        normal_idx=torch.where(dataset.graph_lists[i].ndata['node_labels']==args.normal_class)[0]\n",
    "        abnormal_idx=torch.where(dataset.graph_lists[i].ndata['node_labels']!=args.normal_class)[0]\n",
    "        dataset.graph_lists[i].ndata['node_labels'][normal_idx]=0\n",
    "        dataset.graph_lists[i].ndata['node_labels'][abnormal_idx]=1\n",
    "        \n",
    "\n",
    "        if args.self_loop:\n",
    "            g=dgl.transform.add_self_loop(dataset.graph_lists[i])\n",
    "            g.ndata.update(dataset.graph_lists[i].ndata)\n",
    "            dataset.graph_lists[i]=g\n",
    "\n",
    "        #print(dataset.graph_lists[i].ndata['node_labels'].max())\n",
    "        #print(dataset.graph_lists[i])\n",
    "    return dataset\n",
    "\n",
    "def loader(args):\n",
    "    #if args.dataset == 'PROTEINS_full':\n",
    "    dataset = tu.TUDataset(name=args.dataset)\n",
    "    \n",
    "    train_size = int(0.6 * len(dataset))\n",
    "    #train_size=16\n",
    "    test_size = int(0.25 * len(dataset))\n",
    "    val_size = int(len(dataset) - train_size - test_size)\n",
    "    \n",
    "    dataset = pre_process(args,dataset)\n",
    "\n",
    "    dataset_train, dataset_val, dataset_test = torch.utils.data.random_split(\n",
    "        dataset, (train_size, val_size, test_size))\n",
    "    train_loader = prepare_dataloader(dataset_train, args, train=True)\n",
    "    val_loader = prepare_dataloader(dataset_val, args, train=False)\n",
    "    test_loader = prepare_dataloader(dataset_test, args, train=False)\n",
    "\n",
    "    input_dim,label_dim, max_num_node = dataset.statistics() #I rewrited the code of dgl.tu\n",
    "    print(\"++++++++++STATISTICS ABOUT THE DATASET\")\n",
    "    print(\"dataset feature dimension is\", input_dim)\n",
    "    print(\"dataset label dimension is\", label_dim)\n",
    "    print(\"the max num node is\", max_num_node)\n",
    "    print(\"number of graphs is\", len(dataset))\n",
    "\n",
    "\n",
    "    return train_loader, val_loader, test_loader, input_dim, label_dim\n",
    "\n",
    "\n",
    "def prepare_dataloader(dataset, args, train=False, pre_process=None):\n",
    "    '''\n",
    "    preprocess TU dataset according to DiffPool's paper setting and load dataset into dataloader\n",
    "    '''\n",
    "    if train:\n",
    "        shuffle = True\n",
    "        drop_last = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "        drop_last = False\n",
    "    if pre_process:\n",
    "        pre_process(dataset, args)\n",
    "\n",
    "    # dataset.set_fold(fold)\n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       shuffle=shuffle,\n",
    "                                       collate_fn=batching_graph,\n",
    "                                       drop_last=drop_last,\n",
    "                                       num_workers=args.n_worker)\n",
    "\n",
    "def batching_graph(batch):\n",
    "    '''\n",
    "    for dataset batching\n",
    "    transform ndata to tensor (in gpu is available)\n",
    "    '''\n",
    "    graphs, labels = map(list, zip(*batch))\n",
    "    #cuda = torch.cuda.is_available()\n",
    "\n",
    "    # batch graphs and cast to PyTorch tensor\n",
    "    for graph in graphs:\n",
    "        for (key, value) in graph.ndata.items():\n",
    "            graph.ndata[key] = value.float()\n",
    "    batched_graphs = dgl.batch(graphs)\n",
    "\n",
    "    # cast to PyTorch tensor\n",
    "    batched_labels = torch.LongTensor(np.array(labels))\n",
    "\n",
    "    return batched_graphs, batched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .evaluate import fixed_graph_evaluate, multi_graph_evaluate\n",
    "from .evaluate import thresholding,baseline_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d29224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score,precision_score,recall_score,average_precision_score,roc_auc_score,roc_curve\n",
    "import torch\n",
    "from optim.loss import loss_function,anomaly_score\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "def fixed_graph_evaluate(args,path,model, data_center,data,radius,mask):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        labels = data['labels'][mask]\n",
    "        loss_mask=mask.bool() & data['labels'].bool()\n",
    "\n",
    "        #test_t0 = time.time()\n",
    "        outputs= model(data['g'],data['features'])  \n",
    "\n",
    "\n",
    "        #print(loss_mask.)\n",
    "        _,scores=anomaly_score(data_center,outputs,radius,mask)\n",
    "        #test_dur = time.time()-test_t0\n",
    "        loss,_,_=loss_function(args.nu,data_center,outputs,radius,loss_mask)\n",
    "        #print(\"Test Time {:.4f}\".format(test_dur))\n",
    " \n",
    "        labels=labels.cpu().numpy()\n",
    "        #dist=dist.cpu().numpy()\n",
    "        scores=scores.cpu().numpy()\n",
    "\n",
    "        threshold=0\n",
    "        pred=thresholding(scores,threshold)\n",
    "\n",
    "        auc=roc_auc_score(labels, scores)\n",
    "        ap=average_precision_score(labels, scores)\n",
    "\n",
    "        acc=accuracy_score(labels,pred)\n",
    "        recall=recall_score(labels,pred)\n",
    "        precision=precision_score(labels,pred)\n",
    "        f1=f1_score(labels,pred)\n",
    "\n",
    "        return auc,ap,f1,acc,precision,recall,loss\n",
    "\n",
    "def multi_graph_evaluate(args,path, model, data_center,dataloader,radius,mode='val'):\n",
    "    '''\n",
    "    evaluate function\n",
    "    '''\n",
    "    if mode=='test':\n",
    "        print(f'model loaded.')\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    # pred_list=[]\n",
    "    # labels_list=[]\n",
    "    # scores_list=[]\n",
    "    #correct_label = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_graph, graph_labels) in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                for (key, value) in batch_graph.ndata.items():\n",
    "                    batch_graph.ndata[key] = value.cuda()\n",
    "                #graph_labels = graph_labels.cuda()\n",
    "\n",
    "            # normlizing = nn.InstanceNorm1d(batch_graph.ndata['node_attr'].shape[1], affine=False).cuda()\n",
    "            # input_attr=normlizing(batch_graph.ndata['node_attr'].unsqueeze(1)).squeeze()\n",
    "\n",
    "            normlizing = nn.BatchNorm1d(batch_graph.ndata['node_attr'].shape[1], affine=False).cuda()\n",
    "            input_attr=normlizing(batch_graph.ndata['node_attr'])\n",
    "\n",
    "            outputs = model(batch_graph,input_attr)\n",
    "\n",
    "            labels = batch_graph.ndata['node_labels']\n",
    "            #print(labels.size())\n",
    "            loss_mask=~labels.bool().squeeze()\n",
    "            #print(loss_mask.size())\n",
    "            _,scores=anomaly_score(data_center,outputs,radius,mask=None)\n",
    "            #print(outputs[loss_mask].size())\n",
    "            loss,_,_=loss_function(args.nu,data_center,outputs,radius,loss_mask)\n",
    "\n",
    "            # loss,_,scores=loss_function(args.nu,data_center,outputs,radius,mask=None)\n",
    "            labels=labels.cpu().numpy().astype('int8')\n",
    "            #dist=dist.cpu().numpy()\n",
    "            scores=scores.cpu().numpy()\n",
    "            pred=thresholding(scores,0)\n",
    "            #print('pred',pred[:30])\n",
    "            # print(labels[:10])\n",
    "            # print(scores[:10])\n",
    "\n",
    "            total_loss+=loss\n",
    "            if batch_idx==0:\n",
    "                labels_vec=labels\n",
    "                pred_vec=pred\n",
    "                scores_vec=scores\n",
    "            else:\n",
    "                pred_vec=np.append(pred_vec,pred)\n",
    "                labels_vec=np.concatenate((labels_vec,labels),axis=0)\n",
    "                scores_vec=np.concatenate((scores_vec,scores),axis=0)\n",
    "\n",
    "        total_loss/=(batch_idx+1)\n",
    "        print('score std',scores_vec.std())\n",
    "        print('score mean',scores_vec.mean())\n",
    "        print('labels mean',labels_vec.mean())\n",
    "        print('pred mean',pred_vec.mean())\n",
    "        auc=roc_auc_score(labels_vec, scores_vec)\n",
    "        ap=average_precision_score(labels_vec, scores_vec)\n",
    "\n",
    "        acc=accuracy_score(labels_vec,pred_vec)\n",
    "        recall=recall_score(labels_vec,pred_vec)\n",
    "        precision=precision_score(labels_vec,pred_vec)\n",
    "        f1=f1_score(labels_vec,pred_vec)\n",
    "\n",
    "    return auc,ap,f1,acc,precision,recall,total_loss\n",
    "\n",
    "\n",
    "def thresholding(recon_error,threshold):\n",
    "    ano_pred=np.zeros(recon_error.shape[0])\n",
    "    for i in range(recon_error.shape[0]):\n",
    "        if recon_error[i]>threshold:\n",
    "            ano_pred[i]=1\n",
    "    return ano_pred\n",
    "\n",
    "def baseline_evaluate(datadict,y_pred,y_score,val=True):\n",
    "    \n",
    "    if val==True:\n",
    "        mask=datadict['val_mask']\n",
    "    if val==False:\n",
    "        mask=datadict['test_mask']\n",
    "\n",
    "    auc=roc_auc_score(datadict['labels'][mask],y_score)\n",
    "    ap=average_precision_score(datadict['labels'][mask],y_score)\n",
    "    acc=accuracy_score(datadict['labels'][mask],y_pred)\n",
    "    recall=recall_score(datadict['labels'][mask],y_pred)\n",
    "    precision=precision_score(datadict['labels'][mask],y_pred)\n",
    "    f1=f1_score(datadict['labels'][mask],y_pred)\n",
    "\n",
    "    return auc,ap,f1,acc,precision,recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc,average_precision_score,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ROC(y_test, recon_error_test):    \n",
    "    fpr, tpr, _ = roc_curve(y_test, recon_error_test)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.001, 1])\n",
    "    plt.ylim([0, 1.001])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.savefig('ROC',dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "def plot_PRC(y_test, recon_error_test):\n",
    "    average_precision = average_precision_score(y_test, recon_error_test)\n",
    "\n",
    "    precision,recall,_ = precision_recall_curve(y_test, recon_error_test)\n",
    "\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.4f}'.format(\n",
    "              average_precision))\n",
    "    plt.savefig('PRC',dpi=1200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "            (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random()*N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ac84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = numpy.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return numpy.asarray(all_labels)\n",
    "\n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, embeddings, clf):\n",
    "        self.embeddings = embeddings\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "    def train(self, X, Y, Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        X_train = [self.embeddings[x] for x in X]\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train, Y)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        top_k_list = [len(l) for l in Y]\n",
    "        Y_ = self.predict(X, top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(Y, Y_, average=average)\n",
    "        results['acc'] = accuracy_score(Y,Y_)\n",
    "        print('-------------------')\n",
    "        print(results)\n",
    "        return results\n",
    "        print('-------------------')\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = numpy.asarray([self.embeddings[x] for x in X])\n",
    "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return Y\n",
    "\n",
    "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
    "        state = numpy.random.get_state()\n",
    "\n",
    "        training_size = int(train_precent * len(X))\n",
    "        numpy.random.seed(seed)\n",
    "        shuffle_indices = numpy.random.permutation(numpy.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "        self.train(X_train, Y_train, Y)\n",
    "        numpy.random.set_state(state)\n",
    "        return self.evaluate(X_test, Y_test)\n",
    "\n",
    "\n",
    "def read_node_label(filename, skip_head=False):\n",
    "    fin = open(filename, 'r')\n",
    "    X = []\n",
    "    Y = []\n",
    "    while 1:\n",
    "        if skip_head:\n",
    "            fin.readline()\n",
    "        l = fin.readline()\n",
    "        if l == '':\n",
    "            break\n",
    "        vec = l.strip().split(' ')\n",
    "        X.append(vec[0])\n",
    "        Y.append(vec[1:])\n",
    "    fin.close()\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nxgraph(graph):\n",
    "    node2idx = {}\n",
    "    idx2node = []\n",
    "    node_size = 0\n",
    "    for node in graph.nodes():\n",
    "        node2idx[node] = node_size\n",
    "        idx2node.append(node)\n",
    "        node_size += 1\n",
    "    return idx2node, node2idx\n",
    "\n",
    "\n",
    "def partition_dict(vertices, workers):\n",
    "    batch_size = (len(vertices) - 1) // workers + 1\n",
    "    part_list = []\n",
    "    part = []\n",
    "    count = 0\n",
    "    for v1, nbs in vertices.items():\n",
    "        part.append((v1, nbs))\n",
    "        count += 1\n",
    "        if count % batch_size == 0:\n",
    "            part_list.append(part)\n",
    "            part = []\n",
    "    if len(part) > 0:\n",
    "        part_list.append(part)\n",
    "    return part_list\n",
    "\n",
    "\n",
    "def partition_list(vertices, workers):\n",
    "    batch_size = (len(vertices) - 1) // workers + 1\n",
    "    part_list = []\n",
    "    part = []\n",
    "    count = 0\n",
    "    for v1, nbs in enumerate(vertices):\n",
    "        part.append((v1, nbs))\n",
    "        count += 1\n",
    "        if count % batch_size == 0:\n",
    "            part_list.append(part)\n",
    "            part = []\n",
    "    if len(part) > 0:\n",
    "        part_list.append(part)\n",
    "    return part_list\n",
    "\n",
    "\n",
    "def partition_num(num, workers):\n",
    "    if num % workers == 0:\n",
    "        return [num//workers]*workers\n",
    "    else:\n",
    "        return [num//workers]*workers + [num % workers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import trange\n",
    "\n",
    "from .alias import alias_sample, create_alias_table\n",
    "from .utils import partition_num\n",
    "\n",
    "\n",
    "class RandomWalker:\n",
    "    def __init__(self, G, p=1, q=1):\n",
    "        \"\"\"\n",
    "        :param G:\n",
    "        :param p: Return parameter,controls the likelihood of immediately revisiting a node in the walk.\n",
    "        :param q: In-out parameter,allows the search to differentiate between “inward” and “outward” nodes\n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "\n",
    "    def deepwalk_walk(self, walk_length, start_node):\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = list(self.G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                walk.append(random.choice(cur_nbrs))\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "\n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = list(G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    walk.append(\n",
    "                        cur_nbrs[alias_sample(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    edge = (prev, cur)\n",
    "                    next_node = cur_nbrs[alias_sample(alias_edges[edge][0],\n",
    "                                                      alias_edges[edge][1])]\n",
    "                    walk.append(next_node)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length, workers=1, verbose=0):\n",
    "\n",
    "        G = self.G\n",
    "\n",
    "        nodes = list(G.nodes())\n",
    "\n",
    "        results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
    "            delayed(self._simulate_walks)(nodes, num, walk_length) for num in\n",
    "            partition_num(num_walks, workers))\n",
    "\n",
    "        walks = list(itertools.chain(*results))\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def _simulate_walks(self, nodes, num_walks, walk_length,):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                if self.p == 1 and self.q == 1:\n",
    "                    walks.append(self.deepwalk_walk(\n",
    "                        walk_length=walk_length, start_node=v))\n",
    "                else:\n",
    "                    walks.append(self.node2vec_walk(\n",
    "                        walk_length=walk_length, start_node=v))\n",
    "        return walks\n",
    "\n",
    "    def get_alias_edge(self, t, v):\n",
    "        \"\"\"\n",
    "        compute unnormalized transition probability between nodes v and its neighbors give the previous visited node t.\n",
    "        :param t:\n",
    "        :param v:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        G = self.G\n",
    "        p = self.p\n",
    "        q = self.q\n",
    "\n",
    "        unnormalized_probs = []\n",
    "        for x in G.neighbors(v):\n",
    "            weight = G[v][x].get('weight', 1.0)  # w_vx\n",
    "            if x == t:  # d_tx == 0\n",
    "                unnormalized_probs.append(weight/p)\n",
    "            elif G.has_edge(x, t):  # d_tx == 1\n",
    "                unnormalized_probs.append(weight)\n",
    "            else:  # d_tx > 1\n",
    "                unnormalized_probs.append(weight/q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs = [\n",
    "            float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return create_alias_table(normalized_probs)\n",
    "\n",
    "    def preprocess_transition_probs(self):\n",
    "        \"\"\"\n",
    "        Preprocessing of transition probabilities for guiding the random walks.\n",
    "        \"\"\"\n",
    "        G = self.G\n",
    "\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr].get('weight', 1.0)\n",
    "                                  for nbr in G.neighbors(node)]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs = [\n",
    "                float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = create_alias_table(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "\n",
    "        for edge in G.edges():\n",
    "            alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "\n",
    "        self.alias_nodes = alias_nodes\n",
    "        self.alias_edges = alias_edges\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "class BiasedWalker:\n",
    "    def __init__(self, idx2node, temp_path):\n",
    "\n",
    "        self.idx2node = idx2node\n",
    "        self.idx = list(range(len(self.idx2node)))\n",
    "        self.temp_path = temp_path\n",
    "        pass\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length, stay_prob=0.3, workers=1, verbose=0):\n",
    "\n",
    "        layers_adj = pd.read_pickle(self.temp_path+'layers_adj.pkl')\n",
    "        layers_alias = pd.read_pickle(self.temp_path+'layers_alias.pkl')\n",
    "        layers_accept = pd.read_pickle(self.temp_path+'layers_accept.pkl')\n",
    "        gamma = pd.read_pickle(self.temp_path+'gamma.pkl')\n",
    "        walks = []\n",
    "        initialLayer = 0\n",
    "\n",
    "        nodes = self.idx  # list(self.g.nodes())\n",
    "\n",
    "        results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
    "            delayed(self._simulate_walks)(nodes, num, walk_length, stay_prob, layers_adj, layers_accept, layers_alias, gamma) for num in\n",
    "            partition_num(num_walks, workers))\n",
    "\n",
    "        walks = list(itertools.chain(*results))\n",
    "        return walks\n",
    "\n",
    "    def _simulate_walks(self, nodes, num_walks, walk_length, stay_prob, layers_adj, layers_accept, layers_alias, gamma):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                walks.append(self._exec_random_walk(layers_adj, layers_accept, layers_alias,\n",
    "                                                    v, walk_length, gamma, stay_prob))\n",
    "        return walks\n",
    "\n",
    "    def _exec_random_walk(self, graphs, layers_accept, layers_alias, v, walk_length, gamma, stay_prob=0.3):\n",
    "        initialLayer = 0\n",
    "        layer = initialLayer\n",
    "\n",
    "        path = []\n",
    "        path.append(self.idx2node[v])\n",
    "\n",
    "        while len(path) < walk_length:\n",
    "            r = random.random()\n",
    "            if(r < stay_prob):  # same layer\n",
    "                v = chooseNeighbor(v, graphs, layers_alias,\n",
    "                                   layers_accept, layer)\n",
    "                path.append(self.idx2node[v])\n",
    "            else:  # different layer\n",
    "                r = random.random()\n",
    "                try:\n",
    "                    x = math.log(gamma[layer][v] + math.e)\n",
    "                    p_moveup = (x / (x + 1))\n",
    "                except:\n",
    "                    print(layer, v)\n",
    "                    raise ValueError()\n",
    "\n",
    "                if(r > p_moveup):\n",
    "                    if(layer > initialLayer):\n",
    "                        layer = layer - 1\n",
    "                else:\n",
    "                    if((layer + 1) in graphs and v in graphs[layer + 1]):\n",
    "                        layer = layer + 1\n",
    "\n",
    "        return path\n",
    "\n",
    "\n",
    "def chooseNeighbor(v, graphs, layers_alias, layers_accept, layer):\n",
    "\n",
    "    v_list = graphs[layer][v]\n",
    "\n",
    "    idx = alias_sample(layers_accept[layer][v], layers_alias[layer][v])\n",
    "    v = v_list[idx]\n",
    "\n",
    "    return v\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
